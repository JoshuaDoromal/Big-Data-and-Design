{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48EwlbfxziCl"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install langchain-community\n",
        "!pip install faiss-gpu\n",
        "!pip install pymupdf langchain\n",
        "!pip install pymupdf langchain-openai langchain-core langchain-community\n",
        "!pip install langchain-core\n",
        "!pip install langchain-agents\n",
        "!pip install pillow\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =\"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\""
      ],
      "metadata": {
        "id": "HG_6sZ6GznC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TAVILY_API_KEY'] =\"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\""
      ],
      "metadata": {
        "id": "qeGSgH4czoju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.text_splitter import CharacterTextSplitter  # Correct import for CharacterTextSplitter\n",
        "import re\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import concurrent.futures\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model='gpt-3.5-turbo-0125',\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "search = TavilySearchResults()\n",
        "tools = [search]\n",
        "\n",
        "agent = create_openai_functions_agent(\n",
        "    llm=model,\n",
        "    prompt=prompt,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "agentExecutor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "def process_chat(agentExecutor, user_input, chat_history, pdf_text_chunks):\n",
        "    all_food_items = set()  # Use a set to avoid duplicates\n",
        "    for chunk, store_name in pdf_text_chunks:\n",
        "        food_items = extract_food_items([(chunk, store_name)])\n",
        "        all_food_items.update(food_items)  # Add items to the set\n",
        "\n",
        "    # Convert the set back to a list and filter out generic terms\n",
        "    filtered_food_items = [(item, store) for item, store in all_food_items if item.lower() not in ['product', 'item', 'artikel']]\n",
        "\n",
        "    # Create a numbered list\n",
        "    response_text = \"\\n\".join(f\"{idx + 1}. {item}, {store}\" for idx, (item, store) in enumerate(filtered_food_items))\n",
        "\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    chat_history.append(AIMessage(content=response_text))\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_multiple_pdfs(pdf_paths):\n",
        "    combined_text = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        future_to_pdf = {executor.submit(extract_text_from_pdf, pdf_path): pdf_path for pdf_path in pdf_paths}\n",
        "        for future in concurrent.futures.as_completed(future_to_pdf):\n",
        "            pdf_path = future_to_pdf[future]\n",
        "            try:\n",
        "                data = future.result()\n",
        "                store_name = pdf_path.split('/')[-1]  # Extract store name from the file path\n",
        "                combined_text.append((data, store_name))\n",
        "            except Exception as exc:\n",
        "                print(f\"{pdf_path} generated an exception: {exc}\")\n",
        "    return combined_text\n",
        "\n",
        "def split_text(raw_text, store_name):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "    return [(chunk, store_name) for chunk in text_splitter.split_text(raw_text)]\n",
        "\n",
        "def extract_food_items(text_chunks):\n",
        "    food_items = []\n",
        "    # Define a more specific pattern to avoid generic terms like 'product' and 'item'\n",
        "    pattern = re.compile(r\"\\b(?:voedsel|drank|appel|krieltjes|asperges|lente-ui|rabarber|haakse slijper|volkorenbollen|koekrepen|choco muesli|kaas|eieren|shoarma|vegan shoarma|kaasmix|smeerkaaspuntjes|pancakes|wraps|burrata|koffiebonen|koffiedrank|gehakt|steaks|forelfilets|makreelfilet|kipfiletstukjes|wokgarnalen|zalmfilets|hamburgers|chipolataworstjes|garnalenspies|partysauzen|tortillachips|pommes frites|notenmix|batterijen|handzeep|chicken wings|rucola|perziken|nectarines|watermeloen|kipkerrie|tomaten|roerbakgroenten|pepersteaks|kabeljauwburgers|kiploempia's|muffins|yoghurt|currysnack|rendang|loaded fries|falafel|pizza|bladerdeeghapjes|stoofvlees|gehaktballen|scampi fritti|knoflookboter|knoflookbrood|kimchi|atjar|bakmix|ijs|breekbrood|borrelbroodjes|karamelsiroop)\\b\", re.IGNORECASE)\n",
        "    for chunk, store_name in text_chunks:\n",
        "        matches = pattern.findall(chunk)\n",
        "        for match in matches:\n",
        "            food_items.append((match, store_name))\n",
        "    return food_items\n",
        "\n",
        "def extract_text_from_pdf_with_ocr(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        pix = page.get_pixmap()\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        text += pytesseract.image_to_string(img, lang='nld')  # Use 'nld' for Dutch\n",
        "    return text\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    chat_history = []\n",
        "\n",
        "    # Example PDF paths\n",
        "    pdf_paths = [\n",
        "        \"/content/Folder-Week-20-Landelijk-13-05-20-05-04.pdf\",\n",
        "        \"/content/Folder_ Aldi.pdf\",\n",
        "        \"/content/Albert Heijn - Bonus week 20 2024.pdf\",\n",
        "        \"/content/Folder_Dirk.pdf\",\n",
        "        \"/content/Folder_Jumbo.pdf\"\n",
        "        #\"/content/Another-Folder-Example.pdf\"\n",
        "        # Add more PDF paths if needed\n",
        "    ]\n",
        "\n",
        "    # Extract text from multiple PDFs\n",
        "    combined_pdf_text = extract_text_from_multiple_pdfs(pdf_paths)\n",
        "\n",
        "    # Split the extracted PDF text and include store names\n",
        "    pdf_text_chunks = []\n",
        "    for text, store_name in combined_pdf_text:\n",
        "        pdf_text_chunks.extend(split_text(text, store_name))\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        response = process_chat(agentExecutor, user_input, chat_history, pdf_text_chunks)\n",
        "\n",
        "        print(\"Assistant: \", response)\n",
        "\n",
        "    # Print all extracted food items\n",
        "    print(\"Extracted Food Items: \", response)\n"
      ],
      "metadata": {
        "id": "M5ic-n4MzrE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}