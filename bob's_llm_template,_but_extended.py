# -*- coding: utf-8 -*-
"""Bob's LLM template, but extended

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbNJFz4zzyGg71Tp1TXUF_lPs_tGETv6

# Langchain and Open_AI
The basics in working with chatGPT (an LLM) using Langchain. This is just the start from here you can continue with formatting your prompts and connecting documents or other data to the chat with RAG.

This example uses chatGPT, but you can also use other LLM's, in this OLLAMA is also a good option. This is an offline model, which you download to your computer. You do need to work in Spyder of VSCode to work with it.
"""

pip install langchain

pip install langchain-openai

"""You do need an openai key, which you can get here: https://platform.openai.com/api-keys.
And you need to buy credits. For this you need a credit card. With $5.00 you can get quite far if you use the 3.5 model (set "auto recharge" to off): https://platform.openai.com/account/billing/overview

This is how you set an environmental key which it will save in it's memory


**Assignment: Get an API key and set it below**
"""

import os
os.environ['OPENAI_API_KEY'] ="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"

"""## 1. Intro and setup
https://www.youtube.com/watch?v=ekpnVh-l3YA&list=PL4HikwTaYE0GEs7lvlYJQcvKhq0QZGRVn

"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.4, #closer to 0 is factual, closer to 1 is more creative
    max_tokens = 1000,
    verbose=True
    )



"""Next we use invoke to get a response."""

response = llm.invoke("Who is the 3rd president of the Philippines. Reply in a mix of Tagolog and Enlish, aka Taglish.")
print(response.content)

"""Use llm.stream if you want to stream the content, like chatGPT does. Especially usefull if your answers are long."""

response = llm.batch(["What job pays the best in the Philippines?", "Write a poem about a job that pays the best in the Philippines"])
print(response)

response = llm.stream("Write a poem about a job that pays the best in the Philippines")

for chunk in response:
  print(chunk.content, end="", flush=True)

"""## 2. LLM Chans and Prompt Template
https://www.youtube.com/watch?v=hVs8MVydN3A&list=PL4HikwTaYE0GEs7lvlYJQcvKhq0QZGRVn&index=2

We start of with the basics, creating a prompt template and a chain. In prompt templates you can use variables, like below the variable "subject"

**Assignment: watch the video and finish the code below**
"""

from langchain_core.prompts import ChatPromptTemplate

#create a prompt template
prompt = ChatPromptTemplate.from_template("Tell me a joke about {subject}")

#create a chain
chain = prompt | llm

response = chain.invoke({"subject":"The potential of the socioeconomics of the Philippines"})
print(response.content)

"""Next we will create a different ChatPromptTemplate, which consists out of the list of messages.


**Assignment: create different personalities for the chatbot, how should it respond**
"""

from langchain_core.prompts import ChatPromptTemplate

#create a prompt template
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "generate a list of 10 synonyms for the following word. return the results in a comma separated value file format"),
        ("human", "{input}")
     ]
)

#create a chain
chain = prompt | llm

response = chain.invoke({"input":"cool"})
print(response.content) #.content just leaves the response content, without other additional info (i.e. token size, etc.)

"""## 3. Output Parsers (String, List JSON)

Up until now we retrieved as a response, not only the text, but also a lot of irrelevant info like the model_name, id etc. You can use parsers to determine the format of the response.

https://www.youtube.com/watch?v=qFGdygmYgto&list=PL4HikwTaYE0GEs7lvlYJQcvKhq0QZGRVn&index=3

### Parsing as a String

Let's start wit the StrOutputParser, that gives only the answer back as a string.

**Assignment: add the parsers**
"""

from langchain_core.prompts import ChatPromptTemplate


model = ChatOpenAI(model="gpt-4o", temperature=0.7)

  #create a prompt template
def call_string_output_parser():
    prompt = ChatPromptTemplate.from_messages([
              ("system", "you are a primary school teacher and you want to explain everything in such a way that childeren of 10 years old will also understand"),
              ("human", "{input}")
      ])

    #create a chain
    chain = prompt | model

    return chain.invoke({
          "input": "how to use LangChain"
    })

print(call_string_output_parser())

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def call_list_output_parser():
  #create a prompt template
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "Generate a list of 10 synonyms for the following word. Return the results as a comma separated file"),
          ("human", "{input}")
      ]
  )
  #add a parser
  parser = StrOutputParser()

  #create a chain
  chain = prompt | llm | parser

  return chain.invoke({"input":"Cool"})

print(call_list_output_parser())
print(type(call_list_output_parser()))

"""### Parsing as a Comma separated list

Next we are going to output a list as a comma separated file, using a different parser.
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import CommaSeparatedListOutputParser

def call_list_output_parser():
  #create a prompt template
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "Generate a list of 10 synonyms for the following word. Return the results as a comma separated file"),
          ("human", "{input}")
      ]
  )
  #add a parser
  parser = CommaSeparatedListOutputParser()

  #create a chain
  chain = prompt | llm | parser

  return chain.invoke({"input":"Cool"})

print(call_list_output_parser())
print(type(call_list_output_parser()))

"""### Parsing as JSON

Next we are going to output a list as a comma separated file, using a different parser.


**Assignment: finish the code for the json parser**
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

def call_json_output_parser():
  #create a prompt template
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "Extract information from the following phrase. \nFormatting Instructions: {format_instructions}"),
          ("human", "{phrase}")
      ]
  )

  class Person(BaseModel):
      recipe: str = Field(description="the name of the recipe")
      ingredients: list = Field(description="ingredients")


  #add a parser
  parser = JsonOutputParser(pydantic_object=Person)

  #create a chain
  chain = prompt | llm | parser

  return chain.invoke({"phrase": "The ingredients of adobo are manok, bawang, vinegar, soy sauce and pepper",
                       "format_instructions" : parser.get_format_instructions() })

print(call_json_output_parser())
print(type(call_json_output_parser()))

"""## 4: Chat with Documents using Retrieval Chains

https://youtu.be/-Ueh5XBpcoY?si=vmON10MRH1vMG5pt
"""

!pip install --upgrade langchain

pip install langchain-community

pip install faiss-gpu

#from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains import create_retrieval_chain

def get_documents_from_web(url):
    loader = WebBaseLoader(url)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=20
    )
    splitDocs = splitter.split_documents(docs)
    return splitDocs

def create_db(docs):
    embedding = OpenAIEmbeddings()
    vectorStore = FAISS.from_documents(docs, embedding=embedding)
    return vectorStore

def create_chain(vectorStore):
  model = ChatOpenAI(
      model = "gpt-3.5-turbo-0125",
      temperature = 0.4
  )

  prompt = ChatPromptTemplate.from_template("""
  Answer the user's question:
  Context: {context}
  Question: {input}
  """)

  #chain = prompt | model
  chain = create_stuff_documents_chain(
      llm=model,
      prompt=prompt
  )

  retriever = vectorStore.as_retriever(search_kwargs={"k": 3})

  retrieval_chain = create_retrieval_chain(
      retriever,
      chain
  )

  return retrieval_chain


docs = get_documents_from_web('https://pers.bol.com/en/our-story/#:~:text=Bol%20opened%20its%20doors%20on,(on%201%20january%202024).')
vectorStore = create_db(docs)
chain = create_chain(vectorStore)



response = chain.invoke({
    "input" : "What is bol.com?",
})


print(response["answer"])

#ChatGPT to help with the answer. This entire thing was pasted from the reply


from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

def get_documents_from_web(url):
    loader = WebBaseLoader(url)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=20
    )
    splitDocs = splitter.split_documents(docs)
    return splitDocs

def create_db(docs):
    embedding = OpenAIEmbeddings()
    vectorStore = FAISS.from_documents(docs, embedding=embedding)
    return vectorStore

def create_chain(vectorStore):
    model = ChatOpenAI(
        model="gpt-3.5-turbo-0125",
        temperature=0.4
    )

    prompt = ChatPromptTemplate.from_template("""
    Answer the user's question:
    Context: {context}
    Question: {input}
    """)

    chain = create_stuff_documents_chain(
        llm=model,
        prompt=prompt
    )

    retriever = vectorStore.as_retriever(search_kwargs={"k": 1})

    retrieval_chain = create_retrieval_chain(
        retriever,
        chain
    )

    return retrieval_chain

docs = get_documents_from_web('https://pers.bol.com/en/our-story/#:~:text=Bol%20opened%20its%20doors%20on,(on%201%20january%202024).')

vectorStore = create_db(docs)
chain = create_chain(vectorStore)

response = chain.invoke({
    "input": "What is 1+1?",
})

print(response["answer"])

#how to add more urls, according to chatgpt. it seems to work!

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

def get_documents_from_web(urls):
    all_docs = []
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=20
    )

    for url in urls:
        loader = WebBaseLoader(url)
        docs = loader.load()
        split_docs = splitter.split_documents(docs)
        all_docs.extend(split_docs)

    return all_docs

def create_db(docs):
    embedding = OpenAIEmbeddings()
    vectorStore = FAISS.from_documents(docs, embedding=embedding)
    return vectorStore

def create_chain(vectorStore):
    model = ChatOpenAI(
        model="gpt-3.5-turbo-0125",
        temperature=0.4
    )

    prompt = ChatPromptTemplate.from_template("""
    Answer the user's question:
    Context: {context}
    Question: {input}
    """)

    chain = create_stuff_documents_chain(
        llm=model,
        prompt=prompt
    )

    retriever = vectorStore.as_retriever(search_kwargs={"k": 3})

    retrieval_chain = create_retrieval_chain(
        retriever,
        chain
    )

    return retrieval_chain

# List of URLs to process
urls = [
    'https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/1',
    'https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/2',
    'https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/3',
    'https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/4',
    'https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/5'
    # Add more URLs as needed
    # 'https://example.com'
]

docs = get_documents_from_web(urls)
vectorStore = create_db(docs)
chain = create_chain(vectorStore)

response = chain.invoke({
    "input": "how much are avocadoes and how many pieces??",
})

print(response["answer"])

"""```
# This is formatted as code
```

## 5. Creating a Chatbot with History
Unfortunately, the chatbot does not have memory retention capabilities. For instance, if you tell the chatbot your name and then ask it later to recall your name, it will not remember it.

To solve this issue we will add a chat_history.

### Simple chatbot without history
Create a simple chatbot. Which is not part of the video series, although they do discuss it in video 5, but for a more complicated situation

**Assignment: finish the code and experiment with prompting**
"""

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains import create_retrieval_chain

def get_documents_from_web(url):
    loader = WebBaseLoader(url)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=20
    )
    splitDocs = splitter.split_documents(docs)
    return splitDocs

def create_db(docs):
    embedding = OpenAIEmbeddings()
    vectorStore = FAISS.from_documents(docs, embedding=embedding)
    return vectorStore

def create_chain(vectorStore):
  model = ChatOpenAI(
      model = "gpt-3.5-turbo-0125",
      temperature = 0.4
  )

  prompt = ChatPromptTemplate.from_template("""
  Answer the user's question:
  Context: {context}
  Question: {input}
  """)

  #chain = prompt | model
  chain = create_stuff_documents_chain(
      llm=model,
      prompt=prompt
  )

  retriever = vectorStore.as_retriever(search_kwargs={"k": 3})

  retrieval_chain = create_retrieval_chain(
      retriever,
      chain
  )

  return retrieval_chain


def process_chat(chain, question):
  response = chain.invoke({
      "input" : question,
  })


  print(response["answer"])

if __name__ == '__main__':

  docs = get_documents_from_web('https://www.lidl.nl/l/folders/hah-wk22-2024/view/flyer/page/2')
  vectorStore = create_db(docs)
  chain = create_chain(vectorStore)

  while True:
    user_input = input("You: ")
    if user_input.lower() == 'exit':
        break

    response = process_chat(chain, user_input)
    print("Assistant:", response)

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


#create a prompt template
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a very kind chatbot"),
        ("human", "{input}")
    ]
)
#add a string output parser
parser = StrOutputParser()

#create a chain
chain = prompt | llm |parser



#ask for a prompt
while True:
  user_input = input("You: ")
  if user_input.lower() == 'exit':
    break

  response = chain.invoke({"input":user_input})

  print("Assistant= ", response)

"""**Assignment: create a real chatbot, that prompts the user everytime again. Add this code to the code above:**

![image.png](data:image/png;base64,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)

The chatbot above doesn't have any memory and forgets everything immediatly. That's why we need to give it a memory, so it remembers the chat history and can respond to it.

**Assignment: Spot the difference with the code above and experiment with prompting**
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.messages import HumanMessage,AIMessage


#create a prompt template
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a chotbot from the hiphop community"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ]
)
#add a parser
parser = StrOutputParser()

#create a chain
chain = prompt | llm |parser

#create a chat_history
chat_history =[]

#loop, by everytime giving the user the option to respond, until the user prompts "exit"
while True:
  user_input = input("You: ")
  if user_input.lower() == 'exit':
    break

  response = chain.invoke({
    "input":user_input,
    "chat_history": chat_history
  })

  #add the most recent input and response to the chat_history
  chat_history.append(HumanMessage(content=user_input))
  chat_history.append(AIMessage(content=response))

  print("Assistant= ", response)

"""## LangChain Tutorial (Python) #6: Self-Reasoning Agents with Tools"""

import os
os.environ['TAVILY_API_KEY'] ="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"

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage,AIMessage


model = ChatOpenAI(
  model='gpt-4o',
  temperature=0.6
)

prompt = ChatPromptTemplate.from_messages ([
  ("system", "You are a friendly chef named Gordan Rhamsey with 20 years of experience in making recipes from ingredients."),
  MessagesPlaceholder(variable_name = "chat_history"),
  ("human", "{input}"),
  MessagesPlaceholder(variable_name = "agent_scratchpad")
])

search = TavilySearchResults()
tools = [search]

agent = create_openai_functions_agent(
  llm=model,
  prompt=prompt,
  tools=tools
)


agentExecutor = AgentExecutor(
  agent=agent,
  tools=tools
)

def process_chat(agentExecutor, user_input, chat_history):
  response = agentExecutor.invoke({
      "input": user_input,
      "chat_history" : chat_history
  })

  return response["output"]

if __name__ == '__main__':
  chat_history =[]

  while True:
    user_input = input("You: ")
    if user_input.lower() == 'exit':
      break

    response = process_chat(agentExecutor, user_input, chat_history)

    #add the most recent input and response to the chat_history
    chat_history.append(HumanMessage(content=user_input))
    chat_history.append(AIMessage(content=response))

    print("Assistant= ", response)